{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826f9157-4588-43bb-8eb1-6da360ab9b88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import required libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "690d07ba-96a9-4435-8712-757d89a486ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining the data cleaning methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2282c9e7-7a17-4c09-85a6-74410c80308a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following transformations are required to process the data effectively:\n",
    "#### Pin data:\n",
    "- Replace missing or non-applicable values with None\n",
    "- Adjust the numerical series so they only contain numbers\n",
    "\n",
    "  _e.g. in follower_count '100k' should read '100000'_\n",
    "- Update data types where they are inaccurate\n",
    "- Remove the unnecessary \"Local save in\" prefix from values in the save_location series\n",
    "- Rename 'index' series to match other two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52f59a1-2dc7-48c5-802a-3bbd07db7c21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the clean_pin_df method"
    }
   },
   "outputs": [],
   "source": [
    "def clean_pin_df(df):\n",
    "  \"\"\"\n",
    "  Cleans the pin dataframe by performing the following transformations:\n",
    "  - Replaces the values in the description series with None if they match the values in the to_replace_with_none dictionary\n",
    "  - Converts k, M and B suffixes with 000, 000000 and 000000000 respectively in the follower_count series\n",
    "  - Converts all data types to numeric where applicable\n",
    "  - Removes the unnecessary \"Local save in \" prefix from the save_location series\n",
    "  - Renames the index series to 'ind' to match geo and user dataframes\n",
    "  - Restructures columns in more logical order\n",
    "  \"\"\"\n",
    "  to_replace_with_none = {\n",
    "    'description': ['No description available Story format', 'Untitled', 'No description available Story format'],\n",
    "    'image_src': 'Image src error.',\n",
    "    'poster_name': 'User Info Error',\n",
    "    'tag_list': 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e',\n",
    "    'title': 'No Title Data Available'\n",
    "    }\n",
    "\n",
    "  for column, values in to_replace_with_none.items():\n",
    "    if isinstance(values, list):\n",
    "        for value in values:\n",
    "          cleaned_df = df.replace(value, None, subset=[column])\n",
    "    else:\n",
    "        cleaned_df = cleaned_df.replace(values, None, subset=[column])                    \n",
    "\n",
    "  cleaned_df = cleaned_df.replace({'User Info Error':'0'}, subset=['follower_count'])\n",
    "\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"B\", \"000000000\"))\n",
    "\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", cleaned_df[\"follower_count\"].cast(\"int\"))\n",
    "\n",
    "  cleaned_df = cleaned_df.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "\n",
    "  cleaned_df = cleaned_df.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "  return cleaned_df\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5035626b-68d4-4123-b640-6bc58b38d983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Geo data:\n",
    "- Create a series named 'coordinates' by joining the both the 'latitude' and 'longitude' columns, seperating the values with a comma\n",
    "- Convert the 'timestamp' column to timestamp type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c178825-bb24-45a5-8bc9-55db097e35b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the clean_geo_df method"
    }
   },
   "outputs": [],
   "source": [
    "def clean_geo_df(df):\n",
    "    \"\"\"\n",
    "  Cleans the geo dataframe by performing the following transformations:\n",
    "  - New series 'coordinates' created from latitude and longitude series\n",
    "  - Timestamp series converted to timestamp type\n",
    "  - Restructures columns in more logical order\n",
    "  \"\"\"\n",
    "  cleaned_df = df.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "  cleaned_df = cleaned_df.drop(\"latitude\", \"longitude\")\n",
    "  cleaned_df = cleaned_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "  return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c5b94e-3d1a-4836-b8ba-357157d1761a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### User data\n",
    "- Create a series 'user_name' by joining the 'first_name' and 'last_name' series and then dropping them\n",
    "- Convert 'date_joined' series to timestamp type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41267f9c-4840-49cc-bb59-f98f7dfe87f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the clean_user_df method"
    }
   },
   "outputs": [],
   "source": [
    "def clean_user_df(df):\n",
    "    \"\"\"\n",
    "    Cleans the geo dataframe by performing the following transformations:\n",
    "    - Creates new series 'user_name' created from 'first_name' and 'last_name'\n",
    "    -  Drops 'first_name' and 'last_name' series\n",
    "    - 'date_joined' series converted to timestamp type\n",
    "    - Restructures columns in more logical order\n",
    "    \"\"\"\n",
    "  cleaned_df = df.withColumn(\"user_name\", concat(\"first_name\", lit(\" \"), \"last_name\"))\n",
    "  cleaned_df = cleaned_df.drop(\"first_name\", \"last_name\")\n",
    "  cleaned_df = cleaned_df.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "  return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92bc6be5-d886-4f34-bbf7-76f336c66e75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in the pin dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"s3a://user-12885f560a0b-bucket/topics/12885f560a0b.pin/partition=0/*.json\" \n",
    "file_type = \"json\"  # Corrected file type to match the files being read\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from S3 bucket\n",
    "pin_df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "cleaned_pin_df = clean_pin_df(pin_df)\n",
    "\n",
    "display(cleaned_pin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5c7936-65df-414a-88ea-93dd7cb31737",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in the geo dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"s3a://user-12885f560a0b-bucket/topics/12885f560a0b.geo/partition=0/*.json\" \n",
    "file_type = \"json\"  # Corrected file type to match the files being read\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "geo_df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "\n",
    "cleaned_geo_df = clean_geo_df(geo_df)\n",
    "\n",
    "display(cleaned_geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ebcf09-3116-4b36-8252-c8c540607ebf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Examine data types"
    }
   },
   "outputs": [],
   "source": [
    "geo_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fffce6b8-16ac-458c-91c3-716d60408385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load the data from S3 into Databricks and apply relevant cleaning methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5645ed86-5024-4430-8ac3-8af21d8e6fa2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the user dataframe"
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"s3a://user-12885f560a0b-bucket/topics/12885f560a0b.user/partition=0/*.json\" \n",
    "file_type = \"json\" \n",
    "infer_schema = \"true\"\n",
    "user_df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "\n",
    "cleaned_user_df = clean_user_df(user_df)\n",
    "\n",
    "display(cleaned_user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb208325-b690-4693-8e8d-32bbc378158f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query to find the most popular post category by country\n",
    "This is done by joining the geo_df with the pin_df(using 'ind' as the common series), then grouping by both the category and country columns and applying the `count` aggregation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a27d32-4914-40d6-bd70-7365152208e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Most popular post category by country"
    }
   },
   "outputs": [],
   "source": [
    "joined_df = cleaned_geo_df.join(cleaned_pin_df, cleaned_geo_df.ind == cleaned_pin_df.ind, \"inner\").select(\"country\", \"category\")\n",
    "\n",
    "popular_category_df = joined_df.groupBy(\"country\", \"category\").agg({\"category\": \"count\"}).withColumnRenamed(\"count(category)\", \"category_count\").orderBy(\"category_count\", ascending=False)\n",
    "\n",
    "display(popular_category_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac8e287-8d8b-4986-8121-5aaf2cf39d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query to find the most popular post category by year\n",
    "To do this, the 'post_year' is first extracted from the timestamp into a new series. The dataframe is then grouped with respect to this series and the count aggregator applied like before.\n",
    "##### Sub query: How many posts did each category have between 2018 and 2022?\n",
    "With the 'post_year' series created, this is simply a matter of grouping the dataframe by year, then category and sorting the resulting series from highest to lowest. The result must also be filtered to only include records from 2018-2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f4c1d2-07eb-4b89-b410-44b03cad3127",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Most popular post category by year"
    }
   },
   "outputs": [],
   "source": [
    "joined_df = cleaned_geo_df.join(cleaned_pin_df, cleaned_geo_df.ind == cleaned_pin_df.ind, \"inner\").withColumn(\"post_year\", year(\"timestamp\")).select(\"timestamp\", \"category\")\n",
    "\n",
    "popular_category_by_year_df = joined_df.withColumn(\"post_year\", year(\"timestamp\")).select(\"post_year\", \"category\").groupBy(\"post_year\", \"category\").agg({\"category\": \"count\"}).withColumnRenamed(\"count(category)\", \"category_count\").filter((col(\"post_year\") >= 2018) & (col(\"post_year\") <= 2022)).orderBy(\"category_count\", ascending=False)\n",
    "\n",
    "display(popular_category_by_year_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96453560-737f-448b-8d85-0a1d22771af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query to find the user with the most followers in each country\n",
    "This is a matter of joining the 'geo' and 'pin' dataframes and sorting them by 'follower_count' in descending order.\n",
    "\n",
    "This information can then be used to easily identify the user with the most followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48fc47a-0a7f-4c32-88e5-a9404f91bd76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "User with the most followers in each country"
    }
   },
   "outputs": [],
   "source": [
    "joined_df = cleaned_geo_df.join(cleaned_pin_df, cleaned_geo_df.ind == cleaned_pin_df.ind, \"inner\").select(\"country\", \"poster_name\", \"follower_count\").orderBy(\"follower_count\", ascending=False)\n",
    "\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba09b47-554a-4a17-ac5f-bf82fd31f93c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Country containing user with the most followers"
    }
   },
   "outputs": [],
   "source": [
    "highest_user = joined_df.agg(max(\"follower_count\").alias(\"highest_user\")).collect()[0][\"highest_user\"]\n",
    "\n",
    "result = joined_df.filter(col(\"follower_count\") == highest_user).select(\"country\", \"follower_count\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aa06e1e-7116-4945-bfc0-8debaebe3721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query to find the most popular category for different age groups\n",
    "This is achieved by joining the 'user' and 'pin' dataframes, sorting them by some common age groups, counting the number of records per group for each category, and then sorting the results from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "934a3930-d9c2-418c-b47e-a16ae44f5961",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Most popular category for different age groups"
    }
   },
   "outputs": [],
   "source": [
    "joined_df = cleaned_user_df.join(cleaned_pin_df, cleaned_user_df.ind == cleaned_pin_df.ind, \"inner\").select(\"age\", \"category\")\n",
    "\n",
    "temp_df = joined_df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 18, \"Unspecified\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") < 25), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") < 36), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") < 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ")\n",
    "\n",
    "result = temp_df.select(\"age_group\", \"category\").groupBy(\"age_group\", \"category\").agg({\"category\": \"count\"}).withColumnRenamed(\"count(category)\", \"category_count\").orderBy(\"category_count\", ascending=False)\n",
    "\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efea4718-38e1-41b0-afb9-174478c7e227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query to find the median follower count over different age groups\n",
    "Thankfully, spark has a function for calculating median available in the `functions` library.\n",
    "The syntax is as follows:\n",
    "\n",
    "`\n",
    "df.agg(functions.expr(\"percentile_approx(follower_count, 0.5)\"))\n",
    "`\n",
    "\n",
    "Where the `0.5` refers to the 50th percentile.\n",
    "\n",
    "The dataframe is then grouped by the common age ranges and a new series is created to display the median follower count by age group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a4bf5f-dfcd-40d5-b9ab-da92e34e4597",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Median follower count over different age groups"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "joined_df = cleaned_user_df.join(cleaned_pin_df, cleaned_user_df.ind == cleaned_pin_df.ind, \"inner\").select(\"age\", \"follower_count\").withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 18, \"Unspecified\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") < 25), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") < 36), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") < 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ")\n",
    "temp_df = joined_df.select(\"age_group\", \"follower_count\")\n",
    "agg_df = joined_df.groupBy(\"age_group\") \\\n",
    "    .agg(\n",
    "        F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    "    ).orderBy(\"median_follower_count\", ascending=False)\n",
    "\n",
    "display(agg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77f17c21-6ec5-4ffd-8e50-8e167698b908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query to check how many users joined in each year between 2015 & 2020\n",
    "Achieved by joining the geo and user dataframes, extracting the year from the 'date_joined' series and counting the users by year. Finally the result is filtered to only include records where the 'join_year' is between 2015 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43488595-5567-4202-bf6c-16ccf6ee14ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "How many users joined in each year between 2015 and 2020?"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "joined_df = cleaned_user_df.join(cleaned_geo_df, cleaned_user_df.ind == cleaned_geo_df.ind, \"inner\").select(\"timestamp\", \"date_joined\").withColumn(\"post_year\", year(\"timestamp\")).withColumn(\"join_year\",year(\"date_joined\"))\n",
    "\n",
    "grouped_df = joined_df.groupBy(\"join_year\").agg(count(\"join_year\").alias(\"number_users_joined\")).filter((col(\"join_year\") >= 2015) & (col(\"join_year\") <= 2020)).orderBy(\"join_year\", ascending=True)\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77900603-130a-4d2b-9865-35ba858d1d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query to find the median follower count of users that have joined between 2015 and 2020\n",
    "\n",
    "Again, this makes use of `functions.expr(\"percentile_approx(follower_count, 0.5)\")`, creating a dataframe of median follower counts grouped by join year, once again filtered to exclude records before 2015 or after 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f39a1e0-c9b6-45a7-9cef-ec132d30e09c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Median follower count of users that have joined between 2015 and 2020"
    }
   },
   "outputs": [],
   "source": [
    "joined_df = cleaned_user_df.join(cleaned_pin_df, cleaned_user_df.ind == cleaned_pin_df.ind, \"inner\").select(\"date_joined\",\"follower_count\").withColumn(\"join_year\",year(\"date_joined\"))\n",
    "temp_df = joined_df.select(\"join_year\", \"follower_count\")\n",
    "result = temp_df.groupBy(\"join_year\").agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")).filter((col(\"join_year\") >= 2015) & (col(\"join_year\") <= 2020)).orderBy(\"join_year\", ascending=True)\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7a939a1-e292-4a39-b9ed-946365b9e6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query to find the median follower count of users based on their joining year and age group\n",
    "Similar to the previous query, except now the \"age_group\" series has been reintroduced. By grouping by age range and _then_ by join year, we are able to split the data in the previous dataframe and look at more specific insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b255ef1-3f55-4baf-a2ea-8b9e5defa05a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Median follower count of users based on their joining year and age group"
    }
   },
   "outputs": [],
   "source": [
    "joined_df = cleaned_user_df.join(cleaned_pin_df, cleaned_user_df.ind == cleaned_pin_df.ind, \"inner\").select(\"age\", \"follower_count\", \"date_joined\")\n",
    "\n",
    "temp_df = joined_df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 18, \"Unspecified\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") < 25), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") < 36), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") < 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ").withColumn(\"join_year\",year(\"date_joined\")).select(\"age_group\", \"join_year\", \"follower_count\").filter(\"join_year >= 2015 and join_year <= 2020\")\n",
    "result = temp_df.groupBy(\"age_group\", \"join_year\").agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pinterest Batch Data Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
