{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b26691da-1253-4357-be9a-9017010e24d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Once the stream is running on the client machine:\n",
    "Input AWS username in the `username` field below and run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4acde6da-0e40-4d94-9609-7f550a3a0ad0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set username variable, remove any existing data and reset checkpoint location"
    }
   },
   "outputs": [],
   "source": [
    "username = \"<enter_user_string>\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {username}_pin_table\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {username}_geo_table\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {username}_user_table\")\n",
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2e9555e-0294-482e-9bb1-d6665db63382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following transformations are required to process the data effectively:\n",
    "#### Pin data:\n",
    "- Replace missing or non-applicable values with None\n",
    "- Adjust the numerical series so they only contain numbers\n",
    "\n",
    "  _e.g. in follower_count '100k' should read '100000'_\n",
    "- Update data types where they are inaccurate\n",
    "- Remove the unnecessary \"Local save in\" prefix from values in the save_location series\n",
    "- Rename 'index' series to match other two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914eb9a4-7f04-4616-b7aa-cea655c3602e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define methods for data transformation"
    }
   },
   "outputs": [],
   "source": [
    "def clean_pin_df(df):\n",
    "  \"\"\"\n",
    "  Cleans the pin dataframe by performing the following transformations:\n",
    "  - Replaces the values in the description series with None if they match the values in the to_replace_with_none dictionary\n",
    "  - Converts k, M and B suffixes with 000, 000000 and 000000000 respectively in the follower_count series\n",
    "  - Converts all data types to numeric where applicable\n",
    "  - Removes the unnecessary \"Local save in \" prefix from the save_location series\n",
    "  - Renames the index series to 'ind' to match geo and user dataframes\n",
    "  - Restructures columns in more logical order\n",
    "  \"\"\"\n",
    "  to_replace_with_none = {\n",
    "    'description': ['No description available Story format', 'Untitled', 'No description available Story format'],\n",
    "    'image_src': 'Image src error.',\n",
    "    'poster_name': 'User Info Error',\n",
    "    'tag_list': 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e',\n",
    "    'title': 'No Title Data Available'\n",
    "    }\n",
    "\n",
    "  for column, values in to_replace_with_none.items():\n",
    "    if isinstance(values, list):\n",
    "        for value in values:\n",
    "          cleaned_df = df.replace(value, None, subset=[column])\n",
    "    else:\n",
    "        cleaned_df = cleaned_df.replace(values, None, subset=[column])                    \n",
    "  cleaned_df = cleaned_df.replace({'User Info Error':'0'}, subset=['follower_count'])\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"B\", \"000000000\"))\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", cleaned_df[\"follower_count\"].cast(\"int\"))\n",
    "  cleaned_df = cleaned_df.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "  cleaned_df = cleaned_df.withColumnRenamed(\"index\", \"ind\")\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "  return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec9696d8-ec75-49d7-a9c5-4d280eff1582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Geo data:\n",
    "- Create a series named 'coordinates' by joining the both the 'latitude' and 'longitude' columns, seperating the values with a comma\n",
    "- Convert the 'timestamp' column to timestamp type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e20aaf-fbe3-482b-b6f6-6c646112c6c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_geo_df(df):\n",
    "  \"\"\"\n",
    "  Cleans the geo dataframe by performing the following transformations:\n",
    "  - New series 'coordinates' created from latitude and longitude series\n",
    "  - Timestamp series converted to timestamp type\n",
    "  - Restructures columns in more logical order\n",
    "  \"\"\"\n",
    "  cleaned_df = df.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "  cleaned_df = cleaned_df.drop(\"latitude\", \"longitude\")\n",
    "  cleaned_df = cleaned_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "  return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "434f73e7-c6bd-4134-bde9-d8b2140470a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### User data\n",
    "- Create a series 'user_name' by joining the 'first_name' and 'last_name' series and then dropping them\n",
    "- Convert 'date_joined' series to timestamp type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c28e52b-f907-4d4c-8a83-68c0ea89bfe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_user_df(df):\n",
    "  \"\"\"\n",
    "  Cleans the geo dataframe by performing the following transformations:\n",
    "  - Creates new series 'user_name' created from 'first_name' and 'last_name'\n",
    "  -  Drops 'first_name' and 'last_name' series\n",
    "  - 'date_joined' series converted to timestamp type\n",
    "  - Restructures columns in more logical order\n",
    "  \"\"\"\n",
    "  cleaned_df = df.withColumn(\"user_name\", concat(\"first_name\", lit(\" \"), \"last_name\"))\n",
    "  cleaned_df = cleaned_df.drop(\"first_name\", \"last_name\")\n",
    "  cleaned_df = cleaned_df.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "  return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f59d965e-c658-42a5-800e-374aa5c68a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following is used to bypass format-related errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76442e3c-b088-48a1-a37f-5d7eef2346ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3de706e4-1beb-412c-ba6d-28aaaa138f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, this notebook needs access to the stream. This is done by reading a _separate_ delta table containing the credentials:\n",
    "\n",
    "`delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"`\n",
    "\n",
    "The `Access key ID` and `Secret access key` are assigned to  variables, and the encoded secret key is created.\n",
    "\n",
    "Since ELT is a schema-on-read approach, the schema is defined in the `readStream` function by first assigning it to a python object, and assigning the object as the `schema` parameter.\n",
    "\n",
    "At this stage, it should be possible to read the stream in using the standard syntax. Note that for this particular example, the format is `kinesis` and the `initialPosition` parameter is set to `earliest` to maximise the amount of data that is captured.\n",
    "As soon as the data has been read in, it is immediately written into its corresponding delta table with the `writeStream` method.\n",
    "\n",
    "Once the data has been extracted, it is necessary to run the following to ensure it is in the correct format:\n",
    "```\n",
    "df = df.selectExpr(\"CAST(data as STRING)\")\n",
    "df = df.withColumn(\"data\", from_json(col(\"data\"), schema=<custom_defined_schema>))\n",
    "df = df.select(\"data.*\")\n",
    "```\n",
    "This is repeated for `pin` `geo` and `user` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9f8070-69b6-43ff-81f5-8cbf9375343d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define pin schema, execute clean_pin_df, readStream and writeStream methods for pin_df"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "pin_streaming_schema = StructType([\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"downloaded\", IntegerType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True)]\n",
    "  )\n",
    "\n",
    "pin_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"streaming-12885f560a0b-pin\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .option(\"initialPosition\", \"earliest\") \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    )\n",
    "pin_df = pin_df.selectExpr(\"CAST(data as STRING)\")\n",
    "pin_df = pin_df.withColumn(\"data\", from_json(col(\"data\"), schema=pin_streaming_schema))\n",
    "pin_df = pin_df.select(\"data.*\")\n",
    "\n",
    "cleaned_pin_df = clean_pin_df(pin_df)\n",
    "\n",
    "cleaned_pin_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/pin\") \\\n",
    "  .table(\"12885f560a0b_pin_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7944b61-13c3-4712-a5bf-850dfbd64f27",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define geo schema, execute clean_geo_df, readStream and writeStream methods for geo_df"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "geo_streaming_schema = StructType([\n",
    "  StructField(\"country\", StringType(), True),\n",
    "  StructField(\"ind\", LongType(), True),\n",
    "  StructField(\"latitude\", DoubleType(), True),\n",
    "  StructField(\"longitude\", DoubleType(), True),\n",
    "  StructField(\"timestamp\", StringType(), True)]\n",
    "  )\n",
    "\n",
    "geo_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"streaming-12885f560a0b-geo\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .option(\"initialPosition\", \"earliest\") \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    )\n",
    "geo_df = geo_df.selectExpr(\"CAST(data as STRING)\")\n",
    "geo_df = geo_df.withColumn(\"data\", from_json(col(\"data\"), schema=geo_streaming_schema))\n",
    "geo_df = geo_df.select(\"data.*\")\n",
    "\n",
    "cleaned_geo_df = clean_geo_df(geo_df)\n",
    "\n",
    "cleaned_geo_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/geo\") \\\n",
    "  .table(\"12885f560a0b_geo_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c10d7ffd-3052-4c89-9fde-24e44d80de30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define user schema, execute clean_user_df, readStream and writeStream methods for user_df"
    }
   },
   "outputs": [],
   "source": [
    "user_streaming_schema = StructType([\n",
    "  StructField(\"age\", LongType(), True),\n",
    "  StructField(\"date_joined\", StringType(), True),\n",
    "  StructField(\"first_name\", StringType(), True),\n",
    "  StructField(\"ind\", LongType(), True),\n",
    "  StructField(\"last_name\", StringType(), True)]  \n",
    "  )\n",
    "\n",
    "user_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"streaming-12885f560a0b-user\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .option(\"initialPosition\", \"earliest\") \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    )\n",
    "user_df = user_df.selectExpr(\"CAST(data as STRING)\")\n",
    "user_df = user_df.withColumn(\"data\", from_json(col(\"data\"), schema=user_streaming_schema))\n",
    "user_df = user_df.select(\"data.*\")\n",
    "\n",
    "cleaned_user_df = clean_user_df(user_df)\n",
    "\n",
    "cleaned_user_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/user\") \\\n",
    "  .table(\"12885f560a0b_user_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8562cea-0491-4485-be2e-56ff8042803b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/pin\", True)\n",
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/geo\", True)\n",
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/user\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d33079-14d3-44d4-829b-a17089e4103f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1721684535882720,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Pinterest Streaming Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
