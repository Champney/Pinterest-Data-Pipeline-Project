{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---METHOD HAS BEEN DEPRECATED FROM DATABRICKS---\n",
    "\n",
    "# Code for mounting the AWS S3 bucket(only needs running once)\n",
    "\n",
    "#from pyspark.sql.functions import *\n",
    "#import urllib\n",
    "## define the credentials path\n",
    "#delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "## read the delta table to a Spark DataFrame\n",
    "#\n",
    "#aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "#\n",
    "## get access key and secret access key from spark dataframe\n",
    "#ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "#SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "#\n",
    "## encode the secret access key\n",
    "#\n",
    "#ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "#\n",
    "## AWS S3 bucket name\n",
    "#AWS_S3_BUCKET = \"user-12885f560a0b-bucket\"\n",
    "## Mount name for the bucket\n",
    "#MOUNT_NAME = \"/mnt/mounted-user-12885f560a0b-bucket\"\n",
    "## Source url\n",
    "#SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "## Mount the drive\n",
    "#dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "#\n",
    "## Should output True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbutils.fs.unmount(\"/mnt/mounted-user-12885f560a0b-bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"s3a://user-12885f560a0b-bucket/topics/12885f560a0b.pin/partition=0/*.json\" \n",
    "file_type = \"json\"  # Corrected file type to match the files being read\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from S3 bucket\n",
    "pin_df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(pin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean pin_df:\n",
    "\n",
    "# replace empty entries & entries with no relevant data in each column with None:\n",
    "to_replace_with_none = {\n",
    "    'description': ['No description available Story format', 'Untitled'],\n",
    "    'image_src': 'Image src error.',\n",
    "    'poster_name': 'User Info Error',\n",
    "    'tag_list': 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e',\n",
    "    'title': 'No Title Data Available'\n",
    "}\n",
    "\n",
    "cleaned_pin_df = pin_df\n",
    "\n",
    "for column, values in to_replace_with_none.items():\n",
    "    if isinstance(values, list):\n",
    "        for value in values:\n",
    "            cleaned_pin_df = cleaned_pin_df.replace(value, None, subset=[column])\n",
    "    else:\n",
    "        cleaned_pin_df = cleaned_pin_df.replace(values, None, subset=[column])                    \n",
    "# Using '0' instead of None, as it is a numeric column\n",
    "cleaned_pin_df = cleaned_pin_df.replace({'User Info Error':'0'}, subset=['follower_count'])\n",
    "\n",
    "\n",
    "# Perform the necessary transformations on the follower_count to ensure every entry is a number. Make sure the data type for this column is an int\n",
    "\n",
    "# replace k with 000, M with 000000, and B with 000000000\n",
    "cleaned_pin_df = cleaned_pin_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "cleaned_pin_df = cleaned_pin_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "cleaned_pin_df = cleaned_pin_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"B\", \"000000000\"))\n",
    "\n",
    "# convert to int\n",
    "cleaned_pin_df = cleaned_pin_df.withColumn(\"follower_count\", cleaned_pin_df[\"follower_count\"].cast(\"int\"))\n",
    "\n",
    "# Ensure that each column containing numeric data has a numeric data type\n",
    "\n",
    "# --done\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "cleaned_pin_df = cleaned_pin_df.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "\n",
    "# Rename the index column to ind.\n",
    "cleaned_pin_df = cleaned_pin_df.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder the columns:\n",
    "# ind, unique_id, title, description, follower_count, poster_name, tag_list, is_image_or_video, image_src, save_location, category\n",
    "cleaned_pin_df = cleaned_pin_df.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "#cleaned_pin_df.printSchema()\n",
    "\n",
    "display(cleaned_pin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"s3a://user-12885f560a0b-bucket/topics/12885f560a0b.geo/partition=0/*.json\" \n",
    "file_type = \"json\"  # Corrected file type to match the files being read\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "geo_df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "\n",
    "cleaned_geo_df = geo_df\n",
    "#To clean the df_geo DataFrame you should perform the following transformations:\n",
    "#Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "cleaned_geo_df = cleaned_geo_df.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "#Drop the latitude and longitude columns from the DataFrame\n",
    "cleaned_geo_df = cleaned_geo_df.drop(\"latitude\", \"longitude\")\n",
    "#Convert the timestamp column from a string to a timestamp data type\n",
    "cleaned_geo_df = cleaned_geo_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "#Reorder the DataFrame columns to have the following column order:\n",
    "#ind\n",
    "#country\n",
    "#coordinates\n",
    "#timestamp\n",
    "cleaned_geo_df = cleaned_geo_df.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "# Display Spark dataframe to check its content\n",
    "#cleaned_geo_df.printSchema()\n",
    "display(cleaned_geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"s3a://user-12885f560a0b-bucket/topics/12885f560a0b.user/partition=0/*.json\" \n",
    "file_type = \"json\"  # Corrected file type to match the files being read\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "user_df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "cleaned_user_df = user_df\n",
    "#Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "cleaned_user_df = cleaned_user_df.withColumn(\"user_name\", concat(\"first_name\", lit(\" \"), \"last_name\"))\n",
    "#Drop the first_name and last_name columns from the DataFrame\n",
    "cleaned_user_df = cleaned_user_df.drop(\"first_name\", \"last_name\")\n",
    "#Convert the date_joined column from a string to a timestamp data type\n",
    "cleaned_user_df = cleaned_user_df.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "#Reorder the DataFrame columns to have the following column order:\n",
    "#ind\n",
    "#user_name\n",
    "#age\n",
    "#date_joined\n",
    "cleaned_user_df = cleaned_user_df.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "# Display Spark dataframe to check its content\n",
    "display(cleaned_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 4 : \n",
    "#Find the most popular Pinterest category people post to based on their country.\n",
    "\n",
    "#Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "#country\n",
    "#category\n",
    "#category_count, a new column containing the desired query output\n",
    "\n",
    "# join the dfs\n",
    "joined_df = cleaned_geo_df.join(cleaned_pin_df, cleaned_geo_df.ind == cleaned_pin_df.ind, \"inner\").select(\"country\", \"category\")\n",
    "\n",
    "# Grouping by a column and applying an aggregation function\n",
    "popular_category_df = joined_df.groupBy(\"country\", \"category\").agg({\"category\": \"count\"}).withColumnRenamed(\"count(category)\", \"category_count\").orderBy(\"category_count\", ascending=False)\n",
    "\n",
    "display(popular_category_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 5 : \n",
    "# Find which was the most popular category in each year\n",
    "# Find how many posts each category had between 2018 and 2022.\n",
    "joined_df = cleaned_geo_df.join(cleaned_pin_df, cleaned_geo_df.ind == cleaned_pin_df.ind, \"inner\").withColumn(\"post_year\", year(\"timestamp\")).select(\"timestamp\", \"category\")\n",
    "\n",
    "\n",
    "# Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "# post_year, a new column that contains only the year from the timestamp column\n",
    "# category\n",
    "# category_count, a new column containing the desired query output\n",
    "\n",
    "popular_category_by_year_df = joined_df.withColumn(\"post_year\", year(\"timestamp\")).select(\"post_year\", \"category\").groupBy(\"post_year\", \"category\").agg({\"category\": \"count\"}).withColumnRenamed(\"count(category)\", \"category_count\").filter((col(\"post_year\") >= 2018) & (col(\"post_year\") <= 2022)).orderBy(\"category_count\", ascending=False)\n",
    "\n",
    "display(popular_category_by_year_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 6 : \n",
    "# Find the user with the most followers in each country\n",
    "#Step 1: For each country find the user with the most followers.\n",
    "joined_df = cleaned_geo_df.join(cleaned_pin_df, cleaned_geo_df.ind == cleaned_pin_df.ind, \"inner\").select(\"country\", \"poster_name\", \"follower_count\").orderBy(\"follower_count\", ascending=False)\n",
    "\n",
    "display(joined_df)\n",
    "#popular_category_df = joined_df.groupBy(\"country\", \"category\").agg({\"category\": \"count\"}).withColumnRenamed(\"count(category)\", \"category_count\")\n",
    "\n",
    "#Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "#country\n",
    "#poster_name\n",
    "#follower_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Based on the above query, find the country with the user with most followers.\n",
    "\n",
    "\n",
    "#Your query should return a DataFrame that contains the following columns:\n",
    "highest_user = joined_df.agg(max(\"follower_count\").alias(\"highest_user\")).collect()[0][\"highest_user\"]\n",
    "\n",
    "result = joined_df.filter(col(\"follower_count\") == highest_user).select(\"country\", \"follower_count\")\n",
    "#country\n",
    "#follower_count\n",
    "#This DataFrame should have only one entry.\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 7 : \n",
    "# Find the most popular category for different age groups\n",
    "# What is the most popular category people post to based on the following age groups:\n",
    "\n",
    "#window_spec = Window.partitionBy(\"age\").orderBy()\n",
    "joined_df = cleaned_user_df.join(cleaned_pin_df, cleaned_user_df.ind == cleaned_pin_df.ind, \"inner\").select(\"age\", \"category\")\n",
    "\n",
    "temp_df = joined_df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 18, \"Unspecified\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") < 25), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") < 36), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") < 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ")\n",
    "\n",
    "result = temp_df.select(\"age_group\", \"category\").groupBy(\"age_group\", \"category\").agg({\"category\": \"count\"}).withColumnRenamed(\"count(category)\", \"category_count\").orderBy(\"category_count\", ascending=False)\n",
    "# 18-24\n",
    "# 25-35\n",
    "# 36-50\n",
    "# +50\n",
    "# Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "# age_group, a new column based on the original age column\n",
    "# category\n",
    "# category_count, a new column containing the desired query output\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 8 : \n",
    "# Find the median follower count for the different age groups\n",
    "# What is the median follower count for users in the following age groups:\n",
    "\n",
    "# 18-24\n",
    "# 25-35\n",
    "# 36-50\n",
    "# +50\n",
    "#Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "#age_group, a new column based on the original age column\n",
    "#median_follower_count, a new column containing the desired query output\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "joined_df = cleaned_user_df.join(cleaned_pin_df, cleaned_user_df.ind == cleaned_pin_df.ind, \"inner\").select(\"age\", \"follower_count\").withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 18, \"Unspecified\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") < 25), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") < 36), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") < 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ")\n",
    "temp_df = joined_df.select(\"age_group\", \"follower_count\")\n",
    "agg_df = joined_df.groupBy(\"age_group\") \\\n",
    "    .agg(\n",
    "        F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    "    ).orderBy(\"median_follower_count\", ascending=False)\n",
    "\n",
    "display(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 9 : \n",
    "# Find out how many users have joined each year\n",
    "#Find how many users have joined between 2015 and 2020.\n",
    "\n",
    "\n",
    "#Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "\n",
    "#post_year, a new column that contains only the year from the timestamp column\n",
    "#number_users_joined, a new column containing the desired query output\n",
    "\n",
    "joined_df = cleaned_user_df.join(cleaned_geo_df, cleaned_user_df.ind == cleaned_geo_df.ind, \"inner\").select(\"timestamp\", \"date_joined\").withColumn(\"post_year\", year(\"timestamp\")).withColumn(\"join_year\",year(\"date_joined\"))\n",
    "\n",
    "grouped_df = joined_df.groupBy(\"join_year\").agg(count(\"join_year\").alias(\"number_users_joined\")).filter((col(\"join_year\") >= 2015) & (col(\"join_year\") <= 2020)).orderBy(\"join_year\", ascending=True)\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 10 : \n",
    "# Find the median follower count of users have joined between 2015 and 2020.\n",
    "\n",
    "\n",
    "# Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "# post_year, a new column that contains only the year from the timestamp column\n",
    "# median_follower_count, a new column containing the desired query output\n",
    "joined_df = cleaned_user_df.join(cleaned_pin_df, cleaned_user_df.ind == cleaned_pin_df.ind, \"inner\").select(\"date_joined\",\"follower_count\").withColumn(\"join_year\",year(\"date_joined\"))\n",
    "temp_df = joined_df.select(\"join_year\", \"follower_count\")\n",
    "result = temp_df.groupBy(\"join_year\").agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")).filter((col(\"join_year\") >= 2015) & (col(\"join_year\") <= 2020)).orderBy(\"join_year\", ascending=True)\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 11 : \n",
    "# Find the median follower count of users based on their joining year and age group\n",
    "#Find the median follower count of users that have joined between 2015 and 2020, based on which age group they are part of.\n",
    "\n",
    "\n",
    "#Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "#age_group, a new column based on the original age column\n",
    "#post_year, a new column that contains only the year from the timestamp column\n",
    "#median_follower_count, a new column containing the desired query output\n",
    "\n",
    "joined_df = cleaned_user_df.join(cleaned_pin_df, cleaned_user_df.ind == cleaned_pin_df.ind, \"inner\").select(\"age\", \"follower_count\", \"date_joined\")\n",
    "\n",
    "temp_df = joined_df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 18, \"Unspecified\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") < 25), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") < 36), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") < 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ").withColumn(\"join_year\",year(\"date_joined\")).select(\"age_group\", \"join_year\", \"follower_count\").filter(\"join_year >= 2015 and join_year <= 2020\")\n",
    "result = temp_df.groupBy(\"age_group\", \"join_year\").agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
