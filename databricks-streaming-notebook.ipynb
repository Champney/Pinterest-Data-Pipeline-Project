{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"12885f560a0b\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {username}_pin_table\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {username}_geo_table\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {username}_user_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pin_df(df):\n",
    "  to_replace_with_none = {\n",
    "    'description': ['No description available Story format', 'Untitled', 'No description available Story format'],\n",
    "    'image_src': 'Image src error.',\n",
    "    'poster_name': 'User Info Error',\n",
    "    'tag_list': 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e',\n",
    "    'title': 'No Title Data Available'\n",
    "    }\n",
    "\n",
    "  for column, values in to_replace_with_none.items():\n",
    "    if isinstance(values, list):\n",
    "        for value in values:\n",
    "          cleaned_df = df.replace(value, None, subset=[column])\n",
    "    else:\n",
    "        cleaned_df = cleaned_df.replace(values, None, subset=[column])                    \n",
    "  # Using '0' instead of None, as it is a numeric column\n",
    "  cleaned_df = cleaned_df.replace({'User Info Error':'0'}, subset=['follower_count'])\n",
    "  # replace k with 000, M with 000000, and B with 000000000\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"B\", \"000000000\"))\n",
    "  # Perform the necessary transformations on the follower_count to ensure every entry is a number. Make sure the data type for this column is an int\n",
    "  # convert to int\n",
    "  cleaned_df = cleaned_df.withColumn(\"follower_count\", cleaned_df[\"follower_count\"].cast(\"int\"))\n",
    "  # Ensure that each column containing numeric data has a numeric data type\n",
    "\n",
    "  # Clean the data in the save_location column to include only the save location path\n",
    "  cleaned_df = cleaned_df.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "  # Rename the index column to ind.\n",
    "  cleaned_df = cleaned_df.withColumnRenamed(\"index\", \"ind\")\n",
    "  # Reorder the columns:\n",
    "  # ind, unique_id, title, description, follower_count, poster_name, tag_list, is_image_or_video, image_src, save_location, category\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "  return cleaned_df\n",
    "  \n",
    "def clean_geo_df(df):\n",
    "  cleaned_df = df.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "  cleaned_df = cleaned_df.drop(\"latitude\", \"longitude\")\n",
    "  cleaned_df = cleaned_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "  return cleaned_df\n",
    "\n",
    "def clean_user_df(df):\n",
    "  cleaned_df = df.withColumn(\"user_name\", concat(\"first_name\", lit(\" \"), \"last_name\"))\n",
    "  cleaned_df = cleaned_df.drop(\"first_name\", \"last_name\")\n",
    "  cleaned_df = cleaned_df.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "  cleaned_df = cleaned_df.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "  return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "pin_streaming_schema = StructType([\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"downloaded\", IntegerType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True)]\n",
    "  )\n",
    "\n",
    "geo_streaming_schema = StructType([\n",
    "  StructField(\"country\", StringType(), True),\n",
    "  StructField(\"ind\", LongType(), True),\n",
    "  StructField(\"latitude\", DoubleType(), True),\n",
    "  StructField(\"longitude\", DoubleType(), True),\n",
    "  StructField(\"timestamp\", StringType(), True)]\n",
    "  )\n",
    "\n",
    "user_streaming_schema = StructType([\n",
    "  StructField(\"age\", LongType(), True),\n",
    "  StructField(\"date_joined\", StringType(), True),\n",
    "  StructField(\"first_name\", StringType(), True),\n",
    "  StructField(\"ind\", LongType(), True),\n",
    "  StructField(\"last_name\", StringType(), True)]  \n",
    "  )\n",
    "\n",
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/{username}/pin\", True)\n",
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/{username}/geo\", True)\n",
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/{username}/user\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "pin_streaming_schema = StructType([\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"downloaded\", IntegerType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True)]\n",
    "  )\n",
    "\n",
    "pin_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"streaming-12885f560a0b-pin\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .option(\"initialPosition\", \"earliest\") \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    )\n",
    "pin_df = pin_df.selectExpr(\"CAST(data as STRING)\")\n",
    "pin_df = pin_df.withColumn(\"data\", from_json(col(\"data\"), schema=pin_streaming_schema))\n",
    "pin_df = pin_df.select(\"data.*\")\n",
    "\n",
    "cleaned_pin_df = clean_pin_df(pin_df)\n",
    "\n",
    "cleaned_pin_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/pin\") \\\n",
    "  .table(\"12885f560a0b_pin_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_streaming_schema = StructType([\n",
    "  StructField(\"country\", StringType(), True),\n",
    "  StructField(\"ind\", LongType(), True),\n",
    "  StructField(\"latitude\", DoubleType(), True),\n",
    "  StructField(\"longitude\", DoubleType(), True),\n",
    "  StructField(\"timestamp\", StringType(), True)]\n",
    "  )\n",
    "\n",
    "geo_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"streaming-12885f560a0b-geo\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .option(\"initialPosition\", \"earliest\") \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    )\n",
    "geo_df = geo_df.selectExpr(\"CAST(data as STRING)\")\n",
    "geo_df = geo_df.withColumn(\"data\", from_json(col(\"data\"), schema=geo_streaming_schema))\n",
    "geo_df = geo_df.select(\"data.*\")\n",
    "\n",
    "cleaned_geo_df = clean_geo_df(geo_df)\n",
    "\n",
    "#display(cleaned_geo_df)\n",
    "cleaned_geo_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/geo\") \\\n",
    "  .table(\"12885f560a0b_geo_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_streaming_schema = StructType([\n",
    "  StructField(\"country\", StringType(), True),\n",
    "  StructField(\"ind\", LongType(), True),\n",
    "  StructField(\"latitude\", DoubleType(), True),\n",
    "  StructField(\"longitude\", DoubleType(), True),\n",
    "  StructField(\"timestamp\", StringType(), True)]\n",
    "  )\n",
    "\n",
    "geo_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"streaming-12885f560a0b-geo\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .option(\"initialPosition\", \"earliest\") \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    )\n",
    "geo_df = geo_df.selectExpr(\"CAST(data as STRING)\")\n",
    "geo_df = geo_df.withColumn(\"data\", from_json(col(\"data\"), schema=geo_streaming_schema))\n",
    "geo_df = geo_df.select(\"data.*\")\n",
    "\n",
    "cleaned_geo_df = clean_geo_df(geo_df)\n",
    "\n",
    "#display(cleaned_geo_df)\n",
    "cleaned_geo_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/geo\") \\\n",
    "  .table(\"12885f560a0b_geo_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_streaming_schema = StructType([\n",
    "  StructField(\"age\", LongType(), True),\n",
    "  StructField(\"date_joined\", StringType(), True),\n",
    "  StructField(\"first_name\", StringType(), True),\n",
    "  StructField(\"ind\", LongType(), True),\n",
    "  StructField(\"last_name\", StringType(), True)]  \n",
    "  )\n",
    "\n",
    "user_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"streaming-12885f560a0b-user\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .option(\"initialPosition\", \"earliest\") \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    )\n",
    "user_df = user_df.selectExpr(\"CAST(data as STRING)\")\n",
    "user_df = user_df.withColumn(\"data\", from_json(col(\"data\"), schema=user_streaming_schema))\n",
    "user_df = user_df.select(\"data.*\")\n",
    "\n",
    "cleaned_user_df = clean_user_df(user_df)\n",
    "display(cleaned_user_df)\n",
    "cleaned_user_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/user\") \\\n",
    "  .table(\"12885f560a0b_user_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/pin\", True)\n",
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/geo\", True)\n",
    "dbutils.fs.rm(f\"/tmp/kinesis/_checkpoints/user\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting rid of unwanted objects\n",
    "#dbutils.fs.rm('.../12885f560a0b_user_table_new',recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
